{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a toy dataset used in one of the first diffusion papers [Sohl-Dickstein et.al. 2015], where $$ \\Kset \\subset \\R^2 $$ are points sampled from a spiral. We first construct and visualize this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Swissroll(np.pi/2, 5*np.pi, 100)\n",
    "loader  = DataLoader(dataset, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple dataset, we can implement the denoiser using a multi-layer perceptron (MLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigma_embeds(sigma):\n",
    "    sigma = sigma.unsqueeze(1)\n",
    "    return torch.cat([torch.sin(torch.log(sigma)/2),\n",
    "                      torch.cos(torch.log(sigma)/2)], dim=1)\n",
    "\n",
    "class TimeInputMLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for in_dim, out_dim in pairwise((dim + 2,) + hidden_dims):\n",
    "            layers.extend([nn.Linear(in_dim, out_dim), nn.GELU()])\n",
    "        layers.append(nn.Linear(hidden_dims[-1], dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.input_dims = (dim,)\n",
    "\n",
    "    def rand_input(self, batchsize):\n",
    "        return torch.randn((batchsize,) + self.input_dims)\n",
    "\n",
    "    def forward(self, x, sigma):\n",
    "        sigma_embeds = get_sigma_embeds(sigma)         # shape: b x 2\n",
    "        nn_input = torch.cat([x, sigma_embeds], dim=1) # shape: b x (dim + 2)\n",
    "        return self.net(nn_input)\n",
    "\n",
    "model = TimeInputMLP(dim=2, hidden_dims=(16,128,128,128,128,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP takes the concatenation of $x \\in \\R^2$ and an embedding of the noise level \\sigma, then predicts the noise $\\epsilon \\in \\R^2$\n",
    ". Although many diffusion models use a sinusoidal positional embedding for \n",
    ", the simple two-dimensional embedding works just as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
